import json
import os

import streamlit as st
from dotenv import load_dotenv
from langchain_openai.chat_models import ChatOpenAI
from langgraph.checkpoint.memory import InMemorySaver
from pydantic import SecretStr
from pymilvus import MilvusClient

import db_tools
from prompts.PromptManager import PromptManager
from rag.milvus_helper import search_questions

load_dotenv()

if not os.environ.get("OPENAI_API_KEY") or not os.environ.get("OPENAI_API_BASE"):
    raise ValueError("OPENAI_API_KEY and OPENAI_API_BASE must be set")

config = {"configurable": {"thread_id": "1"}}

checkpointer = InMemorySaver()

from typing import Annotated
from typing_extensions import TypedDict
from langgraph.graph import StateGraph, END

from typing import List, Dict, Any


def custom_append_only(left: List[Dict[str, Any]], right: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    è‡ªå®šä¹‰æ·»åŠ æ–¹æ³•ï¼Œåªå°†æ–°æ¶ˆæ¯è¿½åŠ åˆ°çŠ¶æ€ä¸­
    æ³¨æ„ï¼šleft å‚æ•°åŒ…å«å½“å‰çŠ¶æ€ä¸­çš„æ‰€æœ‰æ¶ˆæ¯
          right å‚æ•°åŒ…å«è¦æ·»åŠ çš„æ–°æ¶ˆæ¯
    """
    # å¦‚æœ right æ˜¯å•ä¸ªå­—å…¸ï¼Œåˆ™è½¬æ¢ä¸ºåˆ—è¡¨
    if not isinstance(right, list):
        right = [right]
    return left + right


class State(TypedDict):
    messages: Annotated[list, custom_append_only]
    pre_call_tolls: list
    is_new_question: bool


pre_call_tolls = []
graph_builder = StateGraph(State)


# æ·»åŠ æ–°çš„åˆ¤æ–­èŠ‚ç‚¹å‡½æ•°
def check_question_relevance(state: State):
    # è·å–å†å²æ¶ˆæ¯å’Œå½“å‰ç”¨æˆ·è¾“å…¥
    messages = state["messages"]
    if not messages:
        return {"is_new_question": True}
    print("1")

    # æå–æœ€è¿‘çš„ç”¨æˆ·æ¶ˆæ¯
    user_messages = [msg for msg in messages if msg.get("role") == "user"]
    print(user_messages)
    if not user_messages or len(user_messages) <= 1:
        return {"is_new_question": True}

    print("2")
    current_question = user_messages[-1]["content"]
    print("3")
    # æ„å»ºç”¨äºåˆ¤æ–­ç›¸å…³æ€§çš„æç¤º
    relevance_prompt = {
        "role": "system",
        "content": "ä½ éœ€è¦åˆ¤æ–­ç”¨æˆ·çš„å½“å‰é—®é¢˜æ˜¯å¦ä¸ä¹‹å‰çš„å¯¹è¯ç›¸å…³ã€‚è¯·è¾“å‡º'ç›¸å…³'æˆ–'ä¸ç›¸å…³'ï¼Œä¸è¦è¾“å‡ºå…¶ä»–å†…å®¹ã€‚"
    }
    # å‡†å¤‡å‘é€ç»™å¤§æ¨¡å‹çš„æ¶ˆæ¯
    history_messages = messages[:-1]  # é™¤äº†å½“å‰ç”¨æˆ·æ¶ˆæ¯å¤–çš„æ‰€æœ‰å†å²æ¶ˆæ¯
    check_relevant = {"role": "user", "content": f"å†å²å¯¹è¯: {history_messages}\n\nå½“å‰é—®é¢˜: {current_question}"}
    chat_messages = [relevance_prompt,check_relevant]
    for msg in chat_messages:
        print(msg)
    try:
        client = OpenAI(
            api_key=os.getenv("DASHSCOPE_API_KEY"),
            base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
        )

        completion = client.chat.completions.create(
            model="qwen3-235b-a22b",
            messages=chat_messages,
            max_tokens=10,
            stream=True,
        )

        result = process_streaming_response(completion)
        answer_content = result["answer_content"]
        print(f"é—®é¢˜ç›¸å…³æ€§åˆ¤æ–­ç»“æœ: {answer_content}")
        # æ ¹æ®ç»“æœè®¾ç½®æ ‡è¯†
        if "ä¸ç›¸å…³" in answer_content:
            return {"is_new_question": True}
        else:
            return {"is_new_question": False}
    except Exception as e:
        print(f"åˆ¤æ–­é—®é¢˜ç›¸å…³æ€§æ—¶å‡ºé”™: {e}")
        # å‡ºé”™æ—¶é»˜è®¤è§†ä¸ºç›¸å…³é—®é¢˜
        return {"is_new_question": False}


# æ·»åŠ ä¸€ä¸ªå¤„ç†æ–°é—®é¢˜çš„èŠ‚ç‚¹
def handle_new_question(state: State):
    user_message = [msg for msg in state["messages"] if msg.get("role") == "user"][-1]
    # æ¸…ç©ºæ¶ˆæ¯
    state["messages"] = []
    question = user_message["content"]
    system_prompt = build_prompt(question)
    system_message = {"role": "system", "content": system_prompt}
    # é‡æ–°æ„å»ºmessagesï¼ŒåªåŒ…å«ç³»ç»Ÿæç¤ºå’Œå½“å‰ç”¨æˆ·è¾“å…¥
    new_messages = [system_message, user_message]
    return {"messages": new_messages}


def route_by_relevance(state: State):
    if state["is_new_question"]:
        return "handle_new_question"
    else:
        return "pre_tools"


from openai import OpenAI
import os


def get_message_role(msg_type: str) -> str:
    if msg_type == "human":
        return "user"
    elif msg_type == "ai":
        return "assistant"
    elif msg_type == "system":
        return "system"
    else:
        return msg_type


def process_streaming_response(completion):
    """
    å¤„ç†OpenAI APIçš„æµå¼è¾“å‡ºå“åº”

    å‚æ•°:
        completion: OpenAI APIè¿”å›çš„æµå¼å“åº”å¯¹è±¡

    è¿”å›:
        dict: åŒ…å«answer_content, reasoning_contentå’Œtool_infoçš„å­—å…¸
    """
    reasoning_content = ""  # å®Œæ•´æ€è€ƒè¿‡ç¨‹
    answer_content = ""  # å®Œæ•´å›å¤
    tool_info = []  # å­˜å‚¨å·¥å…·è°ƒç”¨ä¿¡æ¯

    for chunk in completion:
        if not chunk.choices:
            print("\nUsage:")
            print(chunk.usage)
            continue

        delta = chunk.choices[0].delta
        if hasattr(delta, "content") and delta.content:
            print(delta.content, end="", flush=True)
            answer_content += delta.content
        # åªæ”¶é›†æ€è€ƒå†…å®¹
        if hasattr(delta, "reasoning_content") and delta.reasoning_content is not None:
            print(delta.reasoning_content, end="", flush=True)
            reasoning_content += delta.reasoning_content
        # å¤„ç†å·¥å…·è°ƒç”¨ä¿¡æ¯ï¼ˆæ”¯æŒå¹¶è¡Œå·¥å…·è°ƒç”¨ï¼‰
        if  hasattr(delta,"tool_calls") and delta.tool_calls is not None:
            for tool_call in delta.tool_calls:
                index = tool_call.index  # å·¥å…·è°ƒç”¨ç´¢å¼•ï¼Œç”¨äºå¹¶è¡Œè°ƒç”¨

                # åŠ¨æ€æ‰©å±•å·¥å…·ä¿¡æ¯å­˜å‚¨åˆ—è¡¨
                while len(tool_info) <= index:
                    tool_info.append({})

                # æ”¶é›†å·¥å…·è°ƒç”¨IDï¼ˆç”¨äºåç»­å‡½æ•°è°ƒç”¨ï¼‰
                if tool_call.id:
                    tool_info[index]['id'] = tool_info[index].get('id', '') + tool_call.id

                if tool_call.function and tool_info[index].get("function") is None:
                    tool_info[index]["function"] = {}
                # æ”¶é›†å‡½æ•°åç§°ï¼ˆç”¨äºåç»­è·¯ç”±åˆ°å…·ä½“å‡½æ•°ï¼‰
                if tool_call.function and tool_call.function.name:
                    tool_info[index]["function"]['name'] = tool_info[index]["function"].get('name',
                                                                                            '') + tool_call.function.name

                # æ”¶é›†å‡½æ•°å‚æ•°ï¼ˆJSONå­—ç¬¦ä¸²æ ¼å¼ï¼Œéœ€è¦åç»­è§£æï¼‰
                if tool_call.function and tool_call.function.arguments:
                    tool_info[index]["function"]['arguments'] = tool_info[index]["function"].get('arguments',
                                                                                                 '') + tool_call.function.arguments

    return {
        "answer_content": answer_content,
        "reasoning_content": reasoning_content,
        "tool_info": tool_info
    }


def openai_llm_call(state: State):
    client = OpenAI(
        # å¦‚æœæ²¡æœ‰é…ç½®ç¯å¢ƒå˜é‡ï¼Œè¯·ç”¨é˜¿é‡Œäº‘ç™¾ç‚¼API Keyæ›¿æ¢ï¼šapi_key="sk-xxx"
        api_key=os.getenv("DASHSCOPE_API_KEY"),
        base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
    )

    dict_messages = state["messages"]
    print("openai_llm_call",dict_messages)
    completion = client.chat.completions.create(
        # model="qwen3-32b",
        model="qwen3-235b-a22b",
        messages=dict_messages,
        parallel_tool_calls=True,
        tools=db_tools.tools,
        stream=True,
    )
    result = process_streaming_response(completion)
    answer_content = result["answer_content"]
    tool_info = result["tool_info"]
    print(f"\n" + "=" * 19 + "å·¥å…·è°ƒç”¨ä¿¡æ¯" + "=" * 19)
    if not tool_info:
        print("æ²¡æœ‰å·¥å…·è°ƒç”¨")
    else:
        print(tool_info)
    # å¦‚æœtool_infoä¸ºç©ºå°±ä¸è¦åŠ ä¸Šï¼Œå¦åˆ™æŠ¥é”™ï¼š  Empty tool_calls is not supported in message
    if tool_info:
        return {"messages": [{"role": "assistant", "content": answer_content, "tool_calls": tool_info}]}
    else:
        return {"messages": [{"role": "assistant", "content": answer_content}]}


def route_tools(
        state: State,
):
    """
    Use in the conditional_edge to route to the ToolNode if the last message
    has tool calls. Otherwise, route to the end.
    """
    if isinstance(state, list):
        ai_message = state[-1]
    elif messages := state.get("messages", []):
        ai_message = messages[-1]
    else:
        raise ValueError(f"No messages found in input state to tool_edge: {state}")
    print("route_tools ai_message", ai_message)
    if "tool_calls" in ai_message and len(ai_message["tool_calls"]) > 0:
        print("to tools")
        return "tools"
    print("to end")
    return END


from app_tool import BasicToolNode, ManualToolNode

tool_node = BasicToolNode()
pre_tools_node = ManualToolNode()

graph_builder.add_node("check_relevance", check_question_relevance)
graph_builder.add_node("handle_new_question", handle_new_question)
graph_builder.add_node("llm_call", openai_llm_call)
graph_builder.add_node("tools", tool_node)
graph_builder.add_node("pre_tools", pre_tools_node)

graph_builder.set_entry_point("check_relevance")  # å…¥å£æ”¹ä¸ºåˆ¤æ–­ç›¸å…³æ€§
graph_builder.add_conditional_edges(
    "check_relevance",
    route_by_relevance,
    {"handle_new_question": "handle_new_question", "pre_tools": "pre_tools"}
)
graph_builder.add_edge("handle_new_question", "pre_tools")
graph_builder.add_edge("pre_tools", "llm_call")
graph_builder.add_conditional_edges("llm_call", route_tools, {"tools": "tools", END: END})
graph_builder.add_edge("tools", "llm_call")
graph = graph_builder.compile()

# é¡µé¢é…ç½®
st.set_page_config(
    page_title="LLMå¯¹è¯åŠ©æ‰‹",
    page_icon="ğŸ’¬",
    layout="wide"
)

if "chat_history" not in st.session_state:
    st.session_state.chat_history = []
if "messages" not in st.session_state:
    print("init session_state messages")
    st.session_state.messages = []


def get_vector_store(URI="http://192.168.100.27:19530"):
    client = MilvusClient(uri=URI)
    return client


promptmanager = PromptManager("../background_prompts.md")


def build_prompt(question: str):
    prompt = """
## è§’è‰²
ä½ æ˜¯å…¬å¸å†…éƒ¨çš„è´¢åŠ¡åŠ©æ‰‹ï¼Œç²¾é€šè´¢åŠ¡æŠ¥è¡¨å’Œå‘ç¥¨ç¨åŠ¡çŸ¥è¯†ï¼ŒåŒæ—¶æ“…é•¿ä½¿ç”¨SQLè¯­å¥ã€‚
## ä»»åŠ¡
æ ¹æ®ç”¨æˆ·çš„é—®é¢˜ï¼Œç”Ÿæˆå¹¶æ‰§è¡Œç›¸åº”çš„mysqlè¯­å¥ã€‚ ç”Ÿæˆsqlè¯­å¥åï¼Œéœ€è¦è°ƒç”¨å·¥å…·æ‰§è¡Œsqlè¯­å¥ã€‚   
æŸ¥è¯¢å®Œsqlåï¼Œä½¿ç”¨ä¸€ä¸¤è¡Œå›å¤ç”¨æˆ·æ‰§è¡Œå®Œæˆå³å¯ï¼Œä¸è¦åœ¨å›å¤ä¸­åˆ—å‡ºè¯¦ç»†è®°å½•ã€‚
## æ³¨æ„äº‹é¡¹
æ³¨æ„é€‰æ‹©åˆé€‚çš„å·¥å…·æ¥å®Œæˆä»»åŠ¡ï¼ŒåŒ…æ‹¬æ‰§è¡ŒsqlæŸ¥è¯¢ï¼Œè·å–è¡¨ç»“æ„ç­‰ã€‚   
## è¾“å‡ºæ ¼å¼
è¾“å‡ºç»™ç”¨æˆ·çš„å›ç­”ï¼Œè¯·éµå¾ªmarkdownè¯­æ³•,ç®€è¦æ€»ç»“å³å¯ï¼Œé¿å…æ— å…³è¾“å‡ºï¼Œä¸ç”¨è¾“å‡ºè¯¦ç»†çš„sqlè¯­å¥ï¼Œä¹Ÿä¸ç”¨è¾“å‡ºæŸ¥è¯¢ç»“æœã€‚ 
"""

    entities = search_questions(question)

    hit_question = [entity['question_text'] for entity in entities]

    print("hit_question:", hit_question)
    entity = entities[0]
    category = entity['category']
    print("category:", category)
    category_prompt = promptmanager.get_prompt(category)
    prompt += ("\n## ä¸šåŠ¡èƒŒæ™¯\n")
    prompt += category_prompt

    examples = [
        {"question": entity['question_text'], "explain": entity['question_context'], "sql": entity['example_sql']} for
        entity in entities]
    from langchain.prompts import PromptTemplate, FewShotPromptTemplate
    example_template = """
    ç¤ºä¾‹é—®é¢˜: {question}
    é—®é¢˜åˆ†æ: {explain}
    ç¤ºä¾‹SQL: {sql}
    """
    example_prompt = PromptTemplate(
        input_variables=["question", "explain", "sql"],
        template=example_template,
    )
    prefix = """
## ç¤ºä¾‹é—®é¢˜å’ŒSQLè¯­å¥
"""
    examples_prompt = FewShotPromptTemplate(
        examples=examples,
        example_prompt=example_prompt,
        prefix=prefix,
        suffix="è¯·åˆ†æä¸šåŠ¡èƒŒæ™¯å’Œç¤ºä¾‹é—®é¢˜ï¼Œé€‰æ‹©å’Œç”¨æˆ·é—®é¢˜ç›¸å…³çš„ç¤ºä¾‹ï¼Œç”Ÿæˆæ»¡è¶³ç”¨æˆ·éœ€è¦çš„sqlï¼Œç„¶åè°ƒç”¨å·¥å…·æ‰§è¡Œã€‚å¦‚æœç¤ºä¾‹é—®é¢˜æ²¡æœ‰å’Œç”¨æˆ·é—®é¢˜ç›¸å…³çš„ï¼Œç›´æ¥å›å¤æ ¹æ®ç°æœ‰çŸ¥è¯†ï¼Œæ— æ³•å›ç­”ï¼ã€‚",
        example_separator="\n"  # ç¤ºä¾‹ä¹‹é—´çš„åˆ†éš”ç¬¦
    )

    prompt += examples_prompt.format(input=question)
    print("prompt:", prompt)
    return prompt


# æ¸…é™¤å¯¹è¯å‡½æ•°
def clear_chat():
    # åˆ é™¤session_stateä¸­çš„messages
    # if "messages" in st.session_state:
    #     del st.session_state.messages
    st.session_state.chat_history = []
    st.session_state.messages = []


# é¡µé¢æ ‡é¢˜å’Œè¯´æ˜
st.title("ğŸ’¬ LLMå¯¹è¯åŠ©æ‰‹")

# æ˜¾ç¤ºæ¸…é™¤å¯¹è¯æŒ‰é’®
st.sidebar.button("æ¸…é™¤å¯¹è¯å†å²", on_click=clear_chat, type="primary")

# æ˜¾ç¤ºå¯¹è¯å†å²
for message in st.session_state.chat_history:
    with st.chat_message(message["role"]):
        for component in message["components"]:
            if component["type"] == "code":
                st.code(component["content"])
            elif component["type"] == "text":
                st.text(component["content"])
            elif component["type"] == "title":
                st.title(component["content"])
            elif component["type"] == "error":
                st.error(component["content"])
            elif component["type"] == "subheader":
                st.subheader(component["content"])
            elif component["type"] == "dataframe":
                st.dataframe(component["content"])
            elif component["type"] == "table":
                st.dataframe(component["content"])
            elif component["type"] == "markdown":
                st.markdown(component["content"])
            elif component["type"] == "image":
                st.image(component["content"])
            else:
                st.write(component["content"])

# å¤„ç†ç”¨æˆ·è¾“å…¥
if user_input := st.chat_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜..."):

    print("handle input messages ", [msg.get("role") for msg in st.session_state.messages])
    # å°†ç”¨æˆ·è¾“å…¥æ·»åŠ åˆ°æ¶ˆæ¯åˆ—è¡¨
    st.session_state.messages.append({"role": "user", "content": user_input})

    # æ˜¾ç¤ºç”¨æˆ·è¾“å…¥
    with st.chat_message("user"):
        st.markdown(user_input)
        st.session_state.chat_history.append({"role": "user", "components": [{"type": "text", "content": user_input}]})

    with st.chat_message("assistant"):

        full_response = ""
        history_components = []
        # ä½¿ç”¨streamæ–¹æ³•æ›¿ä»£invokeï¼Œè·å–æµå¼è¾“å‡º
        for chunk in graph.stream({"messages": st.session_state.messages, "pre_call_tolls": pre_call_tolls},
                                  config=config):
            # æ£€æŸ¥æ˜¯å¦æ˜¯æ–°é—®é¢˜ï¼Œå¦‚æœæ˜¯åˆ™æ¸…é™¤èŠå¤©å†å²
            if "check_relevance" in chunk and chunk["check_relevance"].get("is_new_question"):
                print("clear chat ")
                clear_chat()

            # æ¯ä¸ªchunkåŒ…å«å½“å‰æ­¥éª¤çš„ç»“æœ
            print("\n===== ä¸­é—´ç»“æœ =====")
            for key, value in chunk.items():
                print(f"{key}: {value}")
                if "messages" in value:
                    print(f"add {key} messages:")
                    st.session_state.messages.extend(value["messages"])
                if key == "tools":
                    messages = value["messages"]
                    for message in messages:
                        tool_name = message.get("name")
                        content = json.loads(message.get("content"))
                        st.markdown(f"### è°ƒç”¨å·¥å…·ï¼š{tool_name}")
                        if tool_name == "get_all_tables":
                            if "error" in content:
                                st.error(f"é”™è¯¯: {content['error']}")
                                history_components.append({"type": "error", "content": content['error']})
                            else:
                                st.write("æ•°æ®åº“è¡¨åˆ—è¡¨:")
                                history_components.append({"type": "text", "content": "æ•°æ®åº“è¡¨åˆ—è¡¨:"})
                                st.dataframe(content["tables"])
                                history_components.append({"type": "dataframe", "content": content['tables']})
                        elif tool_name == "get_table_schema":
                            if "error" in content:
                                st.error(f"è·å–ç»“æ„é”™è¯¯: {content['error']}")
                                history_components.append({"type": "error", "content": content['error']})
                            else:
                                st.code(content["schema"])
                                history_components.append({"type": "code", "content": content["schema"]})
                        elif tool_name == "get_query_data":
                            if "error" in content:
                                st.error(f"æŸ¥è¯¢é”™è¯¯: {content['error']}")
                                history_components.append({"type": "error", "content": content['error']})
                            else:
                                st.subheader("æŸ¥è¯¢ç»“æœ:")
                                history_components.append({"type": "text", "content": "æŸ¥è¯¢ç»“æœ:"})
                                st.dataframe(content["data"])
                                history_components.append({"type": "dataframe", "content": content["data"]})
                elif key == "llm_call":
                    messages = value["messages"]
                    for message in messages:
                        tool_calls = message.get("tool_calls")
                        if tool_calls is None or len(tool_calls) == 0:
                            st.markdown(message.get("content"))
                        else:
                            for tool_call in tool_calls:
                                func = tool_call["function"]
                                name = func["name"]
                                arguments = json.loads(func["arguments"])
                                if name == "get_query_data":
                                    st.write("æ‰§è¡ŒSQL:")
                                    st.code(arguments["sql"])
                                elif name == "get_table_schema":
                                    st.write(arguments["table_name"])
                else:
                    print(f"å…¶ä»–é”®: {key}")

        st.session_state.chat_history.append({"role": "assistant", "components": history_components})
